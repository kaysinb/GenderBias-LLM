{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from src.training_utils import GenderLossTrainer\n",
    "from src.utils import read_config\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from src.data_utils import prepare_dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.getenv('huggingface_token'))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_configs = read_config('../configs/llm_config.yaml')\n",
    "print(llm_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_configs['local_generative_model_name'])\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_configs['local_generative_model_name'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom gender dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset('../configs/dataset_config.yaml', '../data/short_profession_templates.txt', print_dataset_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_ds_extra_id = -777\n",
    "\n",
    "def format_example(example):\n",
    "    # Basic instruction prompt format\n",
    "    instruction = example[\"context\"]\n",
    "    label = [tokenizer.convert_tokens_to_ids(pr) for pr in example[\"pronoun_list\"]]\n",
    "    input_ids = tokenizer(instruction, truncation=True, max_length=256).input_ids\n",
    "    length = len(input_ids)\n",
    "    label.append(length)\n",
    "    label.append(gender_ds_extra_id)\n",
    "   \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train'].map(format_example, remove_columns=dataset['train'].column_names)\n",
    "dataset_validation = dataset['validation'].map(format_example, remove_columns=dataset['validation'].column_names)\n",
    "dataset_test = dataset['test'].map(format_example, remove_columns=dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.shuffle(seed=42)\n",
    "dataset_train = dataset_train.select(range(len(dataset_train)//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dolly_dataset(max_length=512, print_dataset_info=False):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Dolly dataset to match the custom dataset format.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to encode the texts.\n",
    "        max_length: Maximum sequence length.\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict\n",
    "    \"\"\"\n",
    "    dolly = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "    if print_dataset_info:\n",
    "        print(dolly)\n",
    "        print(dolly[\"train\"][0])\n",
    "    \n",
    "    def preprocess_dolly(example):\n",
    "        instruction = example[\"instruction\"]\n",
    "        context = example.get(\"context\", \"\")\n",
    "        response = example[\"response\"]\n",
    "\n",
    "        if context:\n",
    "            prompt = f\"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            prompt = f\"Instruction:\\n{instruction}\\n\\nAnswer:\"\n",
    "\n",
    "        # Токенизируем разом (потом можно разбить, если хотим)\n",
    "        prompt_ids = tokenizer(prompt, truncation=True, max_length=max_length).input_ids\n",
    "        response_ids = tokenizer(response, truncation=True, max_length=max_length).input_ids\n",
    "        \n",
    "        # Собираем полный вход\n",
    "        input_ids = prompt_ids + response_ids\n",
    "        \n",
    "        # Соответствующие метки\n",
    "        labels = [-100] * len(prompt_ids) + response_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "    dolly = dolly.map(preprocess_dolly, remove_columns=dolly[\"train\"].column_names)\n",
    "    if print_dataset_info:\n",
    "        print(dolly)\n",
    "        print(dolly[\"train\"][0])\n",
    "    \n",
    "    return dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_dataset = load_dolly_dataset(print_dataset_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # Collate input_ids and labels into padded tensors\n",
    "    input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "    labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 for ignored tokens\n",
    "\n",
    "    if input_ids.shape[1] < labels.shape[1]:\n",
    "        input_ids = torch.nn.functional.pad(\n",
    "            input_ids,\n",
    "            (0, labels.shape[1] - input_ids.shape[1]),\n",
    "            value=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    if input_ids.shape[1] > labels.shape[1]:\n",
    "        labels = torch.nn.functional.pad(\n",
    "            labels,\n",
    "            (0, input_ids.shape[1] - labels.shape[1]),\n",
    "            value=-100\n",
    "        )\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": (input_ids != tokenizer.pad_token_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merged_train = concatenate_datasets([dataset_train, dolly_dataset['train']])\n",
    "\n",
    "merged_train = dataset_train ###################\n",
    "merged_train = merged_train.shuffle(seed=42)\n",
    "merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = read_config('../configs/train_config.yaml')\n",
    "\n",
    "target_modules = []\n",
    "\n",
    "for layer_idx in range(train_config['lora_train_config']['layer_numbers']['start'], train_config['lora_train_config']['layer_numbers']['end']):\n",
    "    for proj in train_config['lora_train_config']['layers_to_train']:\n",
    "        target_modules.append(f\"layers.{layer_idx}.{train_config['lora_train_config']['module_to_train']}.{proj}\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=train_config['lora_train_config']['r'],\n",
    "    lora_alpha=train_config['lora_train_config']['lora_alpha'],\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=train_config['lora_train_config']['lora_dropout'],\n",
    "    bias=train_config['lora_train_config']['bias'],\n",
    "    task_type=train_config['lora_train_config']['task_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params\n",
    "\n",
    "\n",
    "# Count parameters in the LoRA-adapted model\n",
    "all_params, trainable_params = count_trainable_params(lora_model)\n",
    "print(f\"All parameters: {all_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Trainable parameters share: {round(trainable_params / all_params * 100, 4)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=\"gender-bias-llm\", config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../test/gender_only_ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir=\"../test/logs\",\n",
    "    report_to=\"wandb\",\n",
    "    # gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "trainer = GenderLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=merged_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=data_collator,\n",
    "    lambda_gender=train_config['lambda_gender'],\n",
    "    gender_ds_extra_id=gender_ds_extra_id\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copa = load_dataset(\"super_glue\", \"copa\")\n",
    "dataset_copa_test = concatenate_datasets([dataset_copa['validation'], dataset_copa['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate_copa(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sample in dataset:\n",
    "        premise = sample[\"premise\"]\n",
    "        question = sample[\"question\"]\n",
    "        choice1 = sample[\"choice1\"]\n",
    "        choice2 = sample[\"choice2\"]\n",
    "        correct_label = sample[\"label\"]\n",
    "        \n",
    "        scores = []\n",
    "        for choice in [choice1, choice2]:\n",
    "            prompt = (f\"Premise: {premise}\\n\"\n",
    "                      f\"Question: What is the most likely {question}?\\n\"\n",
    "                      f\"Choice: {choice}\\n\")\n",
    "            \n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss.item()\n",
    "                \n",
    "            scores.append(-loss)\n",
    "        \n",
    "        pred_label = int(np.argmax(scores))\n",
    "        if pred_label == correct_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_copa(lora_model, tokenizer, dataset_copa_test)\n",
    "print(f\"COPA validation accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
