{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaysinbd/Programs/my_projects/kaysinb/GenderBias-LLM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from src.training_utils import GenderLossTrainer\n",
    "from src.utils import read_config\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from src.data_utils import prepare_dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.getenv('huggingface_token'))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'local_generative_model_name': 'Qwen/Qwen2.5-0.5B-Instruct', 'chatgpt_generative_model_name': 'gpt-4o-2024-11-20', 'gpt_model_to_check_gender': 'gpt-4o-2024-11-20'}\n"
     ]
    }
   ],
   "source": [
    "llm_configs = read_config('../configs/llm_config.yaml')\n",
    "print(llm_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llm_configs['local_generative_model_name'])\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_configs['local_generative_model_name'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom gender dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset info:\n",
      "    Templates: 562\n",
      "    Professions: 60\n",
      "    Dataset rows: 33720\n",
      "Validation dataset info:\n",
      "    Templates: 70\n",
      "    Professions: 6\n",
      "    Dataset rows: 420\n",
      "Test dataset info:\n",
      "    Templates: 71\n",
      "    Professions: 6\n",
      "    Dataset rows: 426\n"
     ]
    }
   ],
   "source": [
    "dataset = prepare_dataset('../configs/dataset_config.yaml', '../data/short_profession_templates.txt', print_dataset_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Write a short story about the electrician\\nGracefully, a electrician handled',\n",
       " 'pronoun_list': ['Ġhis', 'Ġher']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_ds_extra_id = -777\n",
    "\n",
    "def format_example(example):\n",
    "    # Basic instruction prompt format\n",
    "    instruction = example[\"context\"]\n",
    "    label = [tokenizer.convert_tokens_to_ids(pr) for pr in example[\"pronoun_list\"]]\n",
    "    input_ids = tokenizer(instruction, truncation=True, max_length=256).input_ids\n",
    "    length = len(input_ids)\n",
    "    label.append(length)\n",
    "    label.append(gender_ds_extra_id)\n",
    "   \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train'].map(format_example, remove_columns=dataset['train'].column_names)\n",
    "dataset_validation = dataset['validation'].map(format_example, remove_columns=dataset['validation'].column_names)\n",
    "dataset_test = dataset['test'].map(format_example, remove_columns=dataset['test'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.shuffle(seed=42)\n",
    "dataset_train = dataset_train.select(range(len(dataset_train)//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dolly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dolly_dataset(max_length=512, print_dataset_info=False):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Dolly dataset to match the custom dataset format.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to encode the texts.\n",
    "        max_length: Maximum sequence length.\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict\n",
    "    \"\"\"\n",
    "    dolly = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "    if print_dataset_info:\n",
    "        print(dolly)\n",
    "        print(dolly[\"train\"][0])\n",
    "    \n",
    "    def preprocess_dolly(example):\n",
    "        instruction = example[\"instruction\"]\n",
    "        context = example.get(\"context\", \"\")\n",
    "        response = example[\"response\"]\n",
    "\n",
    "        if context:\n",
    "            prompt = f\"Instruction:\\n{instruction}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            prompt = f\"Instruction:\\n{instruction}\\n\\nAnswer:\"\n",
    "\n",
    "        # Токенизируем разом (потом можно разбить, если хотим)\n",
    "        prompt_ids = tokenizer(prompt, truncation=True, max_length=max_length).input_ids\n",
    "        response_ids = tokenizer(response, truncation=True, max_length=max_length).input_ids\n",
    "        \n",
    "        # Собираем полный вход\n",
    "        input_ids = prompt_ids + response_ids\n",
    "        \n",
    "        # Соответствующие метки\n",
    "        labels = [-100] * len(prompt_ids) + response_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "    dolly = dolly.map(preprocess_dolly, remove_columns=dolly[\"train\"].column_names)\n",
    "    if print_dataset_info:\n",
    "        print(dolly)\n",
    "        print(dolly[\"train\"][0])\n",
    "    \n",
    "    return dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_dataset = load_dolly_dataset(print_dataset_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # Collate input_ids and labels into padded tensors\n",
    "    input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "    labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 for ignored tokens\n",
    "\n",
    "    if input_ids.shape[1] < labels.shape[1]:\n",
    "        input_ids = torch.nn.functional.pad(\n",
    "            input_ids,\n",
    "            (0, labels.shape[1] - input_ids.shape[1]),\n",
    "            value=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    if input_ids.shape[1] > labels.shape[1]:\n",
    "        labels = torch.nn.functional.pad(\n",
    "            labels,\n",
    "            (0, input_ids.shape[1] - labels.shape[1]),\n",
    "            value=-100\n",
    "        )\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": (input_ids != tokenizer.pad_token_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merged_train = concatenate_datasets([dataset_train, dolly_dataset['train']])\n",
    "\n",
    "merged_train = dataset_train ###################\n",
    "merged_train = merged_train.shuffle(seed=42)\n",
    "merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = read_config('../configs/train_config.yaml')\n",
    "\n",
    "target_modules = []\n",
    "\n",
    "for layer_idx in range(train_config['lora_train_config']['layer_numbers']['start'], train_config['lora_train_config']['layer_numbers']['end']):\n",
    "    for proj in train_config['lora_train_config']['layers_to_train']:\n",
    "        target_modules.append(f\"layers.{layer_idx}.{train_config['lora_train_config']['module_to_train']}.{proj}\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=train_config['lora_train_config']['r'],\n",
    "    lora_alpha=train_config['lora_train_config']['lora_alpha'],\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=train_config['lora_train_config']['lora_dropout'],\n",
    "    bias=train_config['lora_train_config']['bias'],\n",
    "    task_type=train_config['lora_train_config']['task_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return all_params, trainable_params\n",
    "\n",
    "\n",
    "# Count parameters in the LoRA-adapted model\n",
    "all_params, trainable_params = count_trainable_params(lora_model)\n",
    "print(f\"All parameters: {all_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Trainable parameters share: {round(trainable_params / all_params * 100, 4)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=\"gender-bias-llm\", config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../test/gender_only_ckpt\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir=\"../test/logs\",\n",
    "    report_to=\"wandb\",\n",
    "    # gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "trainer = GenderLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=merged_train,\n",
    "    eval_dataset=dataset_validation,\n",
    "    data_collator=data_collator,\n",
    "    lambda_gender=train_config['lambda_gender'],\n",
    "    gender_ds_extra_id=gender_ds_extra_id,\n",
    "    p_total_power=train_config['p_total_power']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copa = load_dataset(\"super_glue\", \"copa\")\n",
    "dataset_copa_test = concatenate_datasets([dataset_copa['validation'], dataset_copa['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPA validation accuracy: 62.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def evaluate_copa(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sample in dataset:\n",
    "        premise = sample[\"premise\"]\n",
    "        question = sample[\"question\"]\n",
    "        choice1 = sample[\"choice1\"]\n",
    "        choice2 = sample[\"choice2\"]\n",
    "        correct_label = sample[\"label\"]\n",
    "        \n",
    "        scores = []\n",
    "        for choice in [choice1, choice2]:\n",
    "            prompt = (f\"Premise: {premise}\\n\"\n",
    "                      f\"Question: What is the most likely {question}?\\n\"\n",
    "                      f\"Choice: {choice}\\n\")\n",
    "            \n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss.item()\n",
    "                \n",
    "            scores.append(-loss)\n",
    "        \n",
    "        pred_label = int(np.argmax(scores))\n",
    "        if pred_label == correct_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_copa(lora_model, tokenizer, dataset_copa_test)\n",
    "print(f\"COPA validation accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
      "        num_rows: 16113\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
      "        num_rows: 3084\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['goal', 'sol1', 'sol2', 'label'],\n",
      "        num_rows: 1838\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "piqa = load_dataset(\"piqa\")\n",
    "print(piqa)\n",
    "val_data = piqa[\"validation\"]\n",
    "\n",
    "val_data = val_data.shuffle(seed=42)\n",
    "val_data = val_data.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_choice_score(model, tokenizer, prompt, choice_text):\n",
    "    \"\"\"\n",
    "    Computes negative log-likelihood of `choice_text` given the `prompt`.\n",
    "    We'll return *log-prob* (the higher, the more likely).\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Combine the prompt and choice\n",
    "    full_text = prompt + \" \" + choice_text\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # We'll use the model's causal LM head to get the total loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids  # computing cross-entropy over the entire sequence\n",
    "        )\n",
    "        loss = outputs.loss.item()  # average cross-entropy over all tokens\n",
    "\n",
    "    # Return negative loss as \"score\"\n",
    "    # A higher score => lower cross-entropy => better fit\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIQA validation accuracy: 67.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_piqa(model, tokenizer, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for example in dataset:\n",
    "        goal = example[\"goal\"]\n",
    "        sol1 = example[\"sol1\"]\n",
    "        sol2 = example[\"sol2\"]\n",
    "        label = example[\"label\"]  # 0 or 1\n",
    "        \n",
    "        # Construct a simple prompt. For instance:\n",
    "        prompt = f\"Question: {goal}\\nAnswer:\"\n",
    "        \n",
    "        # Score each solution\n",
    "        score_sol1 = compute_choice_score(model, tokenizer, prompt, sol1)\n",
    "        score_sol2 = compute_choice_score(model, tokenizer, prompt, sol2)\n",
    "        \n",
    "        # Predict choice: whichever has higher log-prob\n",
    "        pred_label = 0 if score_sol1 > score_sol2 else 1\n",
    "        \n",
    "        if pred_label == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy_val = evaluate_piqa(lora_model, tokenizer, val_data)\n",
    "print(f\"PIQA validation accuracy: {accuracy_val*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMBADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'domain'],\n",
      "        num_rows: 2662\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'domain'],\n",
      "        num_rows: 5153\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'domain'],\n",
      "        num_rows: 4869\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lambada\")\n",
    "print(dataset)\n",
    "\n",
    "validation_data = dataset[\"validation\"]\n",
    "validation_data = validation_data.shuffle(seed=42)\n",
    "validation_data = validation_data.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"it attached with a long string of sapphires the size of my fist . `` oh , '' she said . `` that reminds me . i saw this piece and thought of you , '' she said , casting about for it . the lady who served us wine the night before handed her what looked like folded velvet . eleanor nodded her thanks and slowly peeled back the layers of velvet\",\n",
       " 'domain': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lambada_next_token_accuracy(model, tokenizer, dataset, max_eval_samples=None):\n",
    "    \"\"\"\n",
    "    For each example, we:\n",
    "      1. Tokenize the entire text.\n",
    "      2. Separate the last token as the 'target'.\n",
    "      3. Feed the preceding tokens (context) into the model.\n",
    "      4. Let the model predict the next token (top-1).\n",
    "      5. Check if it matches the actual last token.\n",
    "    Returns accuracy (#correct / #total).\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        # Convert text to token IDs\n",
    "        tokens = tokenizer.encode(text)\n",
    "        if len(tokens) < 2:\n",
    "            # If the text is too short (only 1 token), skip\n",
    "            continue\n",
    "        \n",
    "        context_ids = tokens[:-1]  # all but last token\n",
    "        target_id = tokens[-1]     # last token\n",
    "\n",
    "        # Convert to tensors\n",
    "        context_ids = torch.tensor([context_ids], dtype=torch.long, device=device)\n",
    "        target_id = torch.tensor([target_id], dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(context_ids)\n",
    "            # outputs.logits shape: (batch, seq_len, vocab_size)\n",
    "            # We want the last hidden state from the final token in context\n",
    "            logits_last = outputs.logits[:, -1, :]  # shape: (batch=1, vocab_size)\n",
    "            pred_id = torch.argmax(logits_last, dim=-1)  # top-1 token index\n",
    "\n",
    "        if pred_id.item() == target_id.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        # Optionally limit number of evaluated samples\n",
    "        if max_eval_samples is not None and (i + 1) >= max_eval_samples:\n",
    "            break\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBADA test accuracy (next-token): 66.20%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_lambada_next_token_accuracy(lora_model, tokenizer, validation_data, max_eval_samples=1000)\n",
    "print(f\"LAMBADA test accuracy (next-token): {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
