lora_train_config:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  modules_to_train:
  - gate_proj
  - up_proj
  - down_proj
  layer_numbers:
    start: 0
    end: 2

lambda_gender: 0.5
