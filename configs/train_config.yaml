lora_train_config:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

  module_to_train: "self_attn"
  layers_to_train:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  layer_numbers:
    start: 0
    end: 2

lambda_gender: 1.5
p_total_power: 1.3
